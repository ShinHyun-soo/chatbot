{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e47e0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "raw_dataset = load_dataset('eaglewatch/Korean_Wikipedia_Dataset_for_GPT2_August_2022')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "108069f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 334420\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 83605\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa741ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>《'''레이디언트 실버건'''》은 트레저가 제작한 1998년 진행형 슈팅 게임이다....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thumb '''2020년 하계 올림픽 아메리칸사모아 선수단'''은 2021년 일본...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>==선== '''선'''(善)이란, 《보살영락본업경》 하권의 〈7. 대중수학품(大衆...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>《'''유아 넥스트'''》(You're Next)는 2011년 공개된 미국의 공포 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>《'''BEST OF SOUL'''》(베스트 오브 소울)은 보아가 일본에서 발매한 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83600</th>\n",
       "      <td>별명: '''백룡부대'''), 또는 '''해병대 제9여단'''은 2015년 12월 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83601</th>\n",
       "      <td>대한민국의 카드사다. == 연혁 == 1986년: '''익스프레스 크레티트 카드''...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83602</th>\n",
       "      <td>'''애덤 로비텔'''(, 1978년 5월 28일 )은 미국의 공포 영화 전문 영화...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83603</th>\n",
       "      <td>'''1993년 코파 오로'''(1993 Copa de Oro)는 1993년 7월 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83604</th>\n",
       "      <td>볼린 '''볼린'''(, )은 폴란드 북서부 서포모제주에 위치한 도시로 면적은 14...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83605 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "0      《'''레이디언트 실버건'''》은 트레저가 제작한 1998년 진행형 슈팅 게임이다....\n",
       "1      thumb '''2020년 하계 올림픽 아메리칸사모아 선수단'''은 2021년 일본...\n",
       "2      ==선== '''선'''(善)이란, 《보살영락본업경》 하권의 〈7. 대중수학품(大衆...\n",
       "3      《'''유아 넥스트'''》(You're Next)는 2011년 공개된 미국의 공포 ...\n",
       "4      《'''BEST OF SOUL'''》(베스트 오브 소울)은 보아가 일본에서 발매한 ...\n",
       "...                                                  ...\n",
       "83600  별명: '''백룡부대'''), 또는 '''해병대 제9여단'''은 2015년 12월 ...\n",
       "83601  대한민국의 카드사다. == 연혁 == 1986년: '''익스프레스 크레티트 카드''...\n",
       "83602  '''애덤 로비텔'''(, 1978년 5월 28일 )은 미국의 공포 영화 전문 영화...\n",
       "83603  '''1993년 코파 오로'''(1993 Copa de Oro)는 1993년 7월 ...\n",
       "83604  볼린 '''볼린'''(, )은 폴란드 북서부 서포모제주에 위치한 도시로 면적은 14...\n",
       "\n",
       "[83605 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset['valid'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4f552ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_dataset = DatasetDict(\n",
    "    {\n",
    "        \"train\": raw_dataset['train'].shuffle(),\n",
    "        \"valid\": raw_dataset['valid'].shuffle()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4353bd40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 334420\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 83605\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2684acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='skt/kogpt2-base-v2', vocab_size=51200, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2')\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54653a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus(ds):\n",
    "    return(\n",
    "        ds[i:i+1000]['text'] for i in range(0, len(ds), 1000)\n",
    "    )\n",
    "\n",
    "training_corpus = get_training_corpus(raw_dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32a77bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer = tokenizer.train_new_from_iterator(training_corpus, vocab_size=50257)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f651fe04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁너', '▁때문에', '▁비행기', '▁시간', '도', '▁', '뺐', '겼', '어']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = \"너 때문에 비행기 시간도 뺐겼어\"\n",
    "\n",
    "tokenizer.tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "383bc0fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [20383, 19253, 36610, 19552, 15318, 3377, 16139, 14770, 16621], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1], 'length': [9]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(sample_text, return_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cc2307c",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 128\n",
    "\n",
    "def tokenize(batch):\n",
    "    outputs = tokenizer(\n",
    "        batch['text'],\n",
    "        max_length=context_length,\n",
    "        truncation=True,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True   \n",
    "    )\n",
    "    \n",
    "    input_batch=[]\n",
    "    for length, input_ids in zip(outputs['length'], outputs['input_ids']):\n",
    "        if length==context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "132dff97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c948d694d6542caa4481f71b44525db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/334420 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1efd5de3bdf148c08768eb3cc4c36766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/83605 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = sampled_dataset.map(tokenize, batched=True, remove_columns=raw_dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd66b196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 1309161\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 323466\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef56c865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('kowiki\\\\tokenizer_config.json',\n",
       " 'kowiki\\\\special_tokens_map.json',\n",
       " 'kowiki\\\\vocab.json',\n",
       " 'kowiki\\\\merges.txt',\n",
       " 'kowiki\\\\added_tokens.json',\n",
       " 'kowiki\\\\tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"kowiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a28200e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 11008,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 32,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_scaling\": null,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"transformers_version\": \"4.32.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LlamaConfig\n",
    "\n",
    "configuration = LlamaConfig()\n",
    "\n",
    "configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04aa5b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(375, 375, 50257)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.bos_token_id, tokenizer.eos_token_id, tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "681e499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = LlamaConfig(**{\n",
    "  \"bos_token_id\": 50256,\n",
    "  \"eos_token_id\": 50256,\n",
    "  \"hidden_act\": \"silu\",\n",
    "  \"hidden_size\": 512,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 1376,\n",
    "  \"max_position_embeddings\": 128,\n",
    "  \"model_type\": \"llama\",\n",
    "  \"num_attention_heads\": 4,\n",
    "  \"num_hidden_layers\": 4,\n",
    "  \"pad_token_id\": 0,\n",
    "  \"rms_norm_eps\": 1e-06,\n",
    "  \"tie_word_embeddings\": False,\n",
    "  \"transformers_version\": \"4.28.0\",\n",
    "  \"use_cache\": True,\n",
    "  \"vocab_size\": 50257\n",
    "}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f462b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(50257, 512, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=512, out_features=1376, bias=False)\n",
       "          (up_proj): Linear(in_features=512, out_features=1376, bias=False)\n",
       "          (down_proj): Linear(in_features=1376, out_features=512, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM\n",
    "\n",
    "model = LlamaForCausalLM(configuration)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11dde3ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0b78d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85d037ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20383, 19253, 36610, 19552, 15318, 20689, 17224, 16621,  3377,   664,\n",
       "           664,   664,   664,   664,   664,   664,   664,   664, 12764, 29428,\n",
       "         11948, 10867, 11948, 10867, 11948, 26969, 26969, 26969, 12764, 12764,\n",
       "         12764, 12764, 12764, 12764, 44613, 44613, 44613, 44613, 44613, 44613,\n",
       "         44613, 44613, 21502, 21502, 21502, 21502, 21502, 21502, 48540, 26969]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"너 때문에 비행기 시간도 놓쳤어 \"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "inputs.to(device)\n",
    "\n",
    "generate_ids = model.generate(inputs.input_ids, max_length=50)\n",
    "generate_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afdcda2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'너 때문에 비행기 시간도 놓쳤어 űűűűűűűűű輈 일종의螯繢螯繢螯 1910년 1910년 1910년輈輈輈輈輈輈 말미암 말미암 말미암 말미암 말미암 말미암 말미암 말미암천군천군천군천군천군천군 오치 1910년'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b3296ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6c4ef2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([3, 128])\n",
      "attention_mask: torch.Size([3, 128])\n",
      "labels: torch.Size([3, 128])\n"
     ]
    }
   ],
   "source": [
    "out = data_collator([tokenized_datasets['train'][i] for i in range(3)])\n",
    "\n",
    "for key in out:\n",
    "    print(f\"{key}: {out[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83c9a7f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([18768, 26053, 15993, 47638, 19078, 22440, 18918, 25691,   441, 33782,\n",
       "         20682, 21388, 19066, 19904, 36632, 19522, 28093, 28320, 36261, 19170]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " tensor([18768, 26053, 15993, 47638, 19078, 22440, 18918, 25691,   441, 33782,\n",
       "         20682, 21388, 19066, 19904, 36632, 19522, 28093, 28320, 36261, 19170]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['input_ids'][0][:20], out['attention_mask'][0][:20], out['labels'][0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f084fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 32\n",
    "logging_steps = 1000\n",
    "learning_rate=5e-4\n",
    "num_epochs=1\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir='kowikimodel',\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=logging_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    save_steps=logging_steps,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=logging_steps,\n",
    "    lr_scheduler_type='cosine',\n",
    "    learning_rate=5e-4,\n",
    "    fp16=True,\n",
    "    push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60143b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['valid']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb730e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20456' max='20456' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20456/20456 8:53:53, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>7.961300</td>\n",
       "      <td>6.150519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>5.435100</td>\n",
       "      <td>5.091812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.913200</td>\n",
       "      <td>4.831055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.722000</td>\n",
       "      <td>4.691142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>4.603100</td>\n",
       "      <td>4.597617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>4.464500</td>\n",
       "      <td>4.537406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>4.415800</td>\n",
       "      <td>4.478937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>4.370400</td>\n",
       "      <td>4.430090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>4.327400</td>\n",
       "      <td>4.385536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>4.289100</td>\n",
       "      <td>4.346845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>4.179100</td>\n",
       "      <td>4.324713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>4.150400</td>\n",
       "      <td>4.292959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>4.123600</td>\n",
       "      <td>4.263522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>4.106800</td>\n",
       "      <td>4.237631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>4.086300</td>\n",
       "      <td>4.213760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>4.006300</td>\n",
       "      <td>4.206041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>3.968500</td>\n",
       "      <td>4.194574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>3.965900</td>\n",
       "      <td>4.184756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>3.958200</td>\n",
       "      <td>4.179421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>3.954400</td>\n",
       "      <td>4.177619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20456, training_loss=4.487922886269233, metrics={'train_runtime': 32035.1339, 'train_samples_per_second': 163.466, 'train_steps_per_second': 0.639, 'total_flos': 1.5437314156658688e+17, 'train_loss': 4.487922886269233, 'epoch': 4.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d85cb661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[28838, 39420,  3737, 19862, 18823, 17894, 16555, 19396, 20778, 18804,\n",
       "         14698, 16282, 23353,  3737, 19862, 18823, 17894, 16555, 19396, 20778,\n",
       "         18804, 14698, 16282, 23353,  3737, 19862, 18823, 17894, 16555, 19396,\n",
       "         20778, 18804, 14698, 16282, 23353,  3737, 19862, 18823, 17894, 16555,\n",
       "         19396, 20778, 18804, 14698, 16282, 23353,  3737, 19862, 18823, 17894]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"안녕\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "inputs.to(device)\n",
    "\n",
    "generate_ids = model.generate(inputs.input_ids, max_length=50)\n",
    "generate_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "37f5ba34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요》 ... 나훈아 2014년 KBS2 《감성세대》 ... 나훈아 2014년 KBS2 《감성세대》 ... 나훈아 2014년 KBS2 《감성세대》 ... 나훈아 2014년 KBS2 《감성세대》 ... 나훈'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1626fa10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
